{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7424bccd",
   "metadata": {},
   "source": [
    "# \"Machine learning con Python (en construcción)\"\n",
    "> \"Guía de referencia rápida de machine learning con Python\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [python, kaggle, ML]\n",
    "- image: images/python.png\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77efb9f",
   "metadata": {},
   "source": [
    "> tip: Puedes ver este post en GitHub o ejecutarlo en Binder o Google Colab, pulsa el icono."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5d3f3d",
   "metadata": {},
   "source": [
    "Fuentes:<br>\n",
    "[Cursos Machine Learning - Kaggle](https://www.kaggle.com/learn)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df811a4",
   "metadata": {},
   "source": [
    "## 1 - Recoger los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac7fbf8",
   "metadata": {},
   "source": [
    "## 2 - Preparar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d829f",
   "metadata": {},
   "source": [
    "### 2.1 - Exploración de los datos\n",
    "Importante leer la documentación de los datos y estudiarlos bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aed429b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ruta_archivo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4097/2228622085.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdatos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruta_archivo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# cargar datos desde tabla en archivo csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdatos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# imprime resumen de los datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdatos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m                     \u001b[0;31m# imprime lista de las columnas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ruta_archivo' is not defined"
     ]
    }
   ],
   "source": [
    "#collapse-output\n",
    "import pandas as pd\n",
    "\n",
    "datos = pd.read_csv(ruta_archivo) # cargar datos desde tabla en archivo csv\n",
    "datos.describe()                  # imprime resumen de los datos\n",
    "datos.columns                     # imprime lista de las columnas\n",
    "datos.head()                      # imprime lista de las 5 primeras muestras de la tabla\n",
    "datos.tail()                      # imprime lista de las 5 últimas muestras de la tabla\n",
    "\n",
    "datos_filtrados = datos.dropna(axis=0) # elimina datos na (no available - no disponibles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf7852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-output\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True) # Eliminar filas sin datos destino, \n",
    "\n",
    "# elegir la columna que deseamos estimar en nuestro modelo\n",
    "y = X_full.SalePrice\n",
    "\n",
    "# elegir las columnas que se pasarán al modelo\n",
    "# opción 1: quitar la columna destino\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# opción 2: elegir sólo algunas columnas\n",
    "datos_features = ['Rooms', 'Bathroom','Lattitude', 'Longtitude'] \n",
    "X = datos[datos_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3125f7",
   "metadata": {},
   "source": [
    "### 2.2- Tratamiento de valores que faltan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d1c5d3",
   "metadata": {},
   "source": [
    "¿Por qué faltan los valores?\n",
    "  - No existen -> no hacer nada, no tiene sentido intentar estimarlos\n",
    "  - No han sido grabados -> se puede intentar estimarlos, basándonos en otros valores de la tabla\n",
    "  \n",
    "Revisar cada columna para averiguar cual es la mejor aproximación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce36e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_val_count_by_column = X_train.isnull().sum() # Número de valores faltantes en cada columna de datos de entrenamiento \n",
    "\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
    "\n",
    "# how many total missing values do we have?\n",
    "total_cells = np.product(X_train.shape)\n",
    "total_missing = missing_val_count_by_column.sum()\n",
    "\n",
    "# percent of data that is missing\n",
    "percent_missing = (total_missing/total_cells) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5698d6b",
   "metadata": {},
   "source": [
    "#### 2.2.1 - Eliminar columna\n",
    "Para el caso en falten la mayoría de datos de una variable. Sino se puede perder información importante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_missing = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]       # recopilar columnas con valores que faltan\n",
    "\n",
    "# eliminar columnas con valores que faltan\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)  \n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_X_train= X_train.dropna()  # Otra opción: elimina todas las columnas con datos no disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e2bfc3",
   "metadata": {},
   "source": [
    "#### 2.2.2 - Imputación\n",
    "Rellenar los valores faltantes con otro valor, por ejemplo: la media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e95063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputación\n",
    "my_imputer = SimpleImputer(strategy='median')  # mean, median, most_frequent, constant\n",
    "\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train), index=range(1, X_train.shape[0] + 1),\n",
    "                          columns=range(1, X_train.shape[1] + 1))  \n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid), index=range(1, X_valid.shape[0] + 1),\n",
    "                          columns=range(1, X_valid.shape[1] + 1))  \n",
    "\n",
    "# Imputación quitó los nombres de columnas, volver a ponerlos \n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f48a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fillna(0)  # reemplazar con 0 los NA\n",
    "X_train.fillna(method='bfill', axis=0).fillna(0) # reemplaza también los valores que vienen después en la misma columna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5373b010",
   "metadata": {},
   "source": [
    "#### 2.2.3 - Extensión de Imputación\n",
    "Añadiendo otra columna que informe si faltaba o no el valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017a42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_plus = X_train.copy() # Hacer una copia para evitar cambiar los datos originales (al imputar) \n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Hacer nuevas columnas que indiquen lo que se imputará. \n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "# Imputación\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "# Imputación quitó los nombres de columnas, volver a ponerlos \n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604069bd",
   "metadata": {},
   "source": [
    "### 2.3.- Variables categóricas\n",
    "Toman un número limitado de valores. Necesitan ser preprocesados antes de usarlos en los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804f1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "# para obtener lista de variables categóricas \n",
    "\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dccfbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_label_cols = [col for col in object_cols if   # Columnas que se pueden codificar con etiquetas de forma segura \n",
    "                   set(X_train[col]) == set(X_valid[col])]\n",
    "        \n",
    "# Columnas problemáticas que se eliminarán del conjunto de datos\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "\n",
    "# Aplicar codificador de etiquetas \n",
    "label_encoder = LabelEncoder()\n",
    "for col in good_label_cols:\n",
    "    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "    label_X_valid[col] = label_encoder.transform(X_valid[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols)) \n",
    "# Obtener el número de entradas únicas en cada columna con datos categóricos \n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# Imprimirc el número de entradas únicas por columna, en orden ascendente \n",
    "sorted(d.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7446af",
   "metadata": {},
   "source": [
    "#### 2.3.1- Eliminar columna\n",
    "Sólo es buena opción si la columna no tiene información útil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20deed7e",
   "metadata": {},
   "source": [
    "#### 2.3.2- Codificación ordinal\n",
    "Asigna un entero a cada valor único. Asume un orden en las categorías. Variables ordinales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f891850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Hacer una copia para evitar cambiar los datos originales \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Aplicar codificador ordinal a cada columna con datos categóricos \n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n",
    "\n",
    "# De esta forma ha asigando valores aleatorios a las categorías. Asignar un orden puede mejorar el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dba9c7",
   "metadata": {},
   "source": [
    "#### 2.3.3- Codificación *One-Hot*\n",
    "Crea una columna por cada valor único e indica la presencia o no de ese valor. No asume un orden en las categorías. Variables nominales. No funciona bien si hay muchas categorias. Usar si hay 15 como máximo.\n",
    "\n",
    "* handle_unknown='ignore': para evitar errores cuando los datos de validación contienen clases que no están representadas en los datos de entrenamiento \n",
    "* sparse=False: asegura que las columnas codificadas se devuelvan como una matriz densa (en lugar de una matriz dispersa)\n",
    "\n",
    "Suele ser la mejor aproximación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f1827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Columnas que se codificarán one-hot (como máximo 10 categorias)\n",
    "low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "# Columnas que se eliminarán\n",
    "high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "\n",
    "\n",
    "# Aplicar codificador one-hot a cada columna con datos categóricos\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n",
    "\n",
    "# codificador one-hot elimina; ponerlo de nuevo \n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Eliminar columnas categóricas (se reemplazarán con codificación one-hot) \n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "#  añadir columnas codificadas one-hot a variables numéricas \n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d11869b",
   "metadata": {},
   "source": [
    "### 2.3- Codificaciones de caracteres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4497734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "after = before.encode(\"utf-8\", errors=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c678e34c",
   "metadata": {},
   "source": [
    "Detectar codificación automáticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c5b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data.csv\", 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(10000))  # en este caso lee 10.000 caracteres\n",
    "    \n",
    "print(result['encoding'])\n",
    "datos = pd.read_csv(\"./data.csv\", encoding=result['encoding']) # guardar en archivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545372e5",
   "metadata": {},
   "source": [
    "### 2.4- Escalado y normalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3706f1",
   "metadata": {},
   "source": [
    "#### Escalado\n",
    "Cambia el rango de los datos. Por ejemplo, pasar datos a un rango 0-1 o 0-100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import minmax_scaling\n",
    "\n",
    "scaled_data = minmax_scaling(original_data, columns=[0]) #  escalado entre 0 y 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f76f90",
   "metadata": {},
   "source": [
    "#### Normalizado\n",
    "Cambia la forma del histograma para que sea una distribución normal (Gaussiana):\n",
    "- Media = mediana = moda\n",
    "- Forma de campana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# obtener el índice de todos los valores positivos (Box-Cox solo acepta valores positivos) \n",
    "index_of_positives = datos.pledged > 0\n",
    "\n",
    "# get only positive pledges (using their indexes)\n",
    "valores_positivos = datos.columnaA.loc[index_of_positives]\n",
    "\n",
    "# normalizar con Box-Cox\n",
    "norm_valores = pd.Series(stats.boxcox(valores_positivos)[0], \n",
    "                               name='columnaA', index=valores_positivos.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3acde80",
   "metadata": {},
   "source": [
    "### 2.5- Fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "datos['nueva_columna'] = pd.to_datetime(datos['fecha'], format=\"%d/%m/%y\",infer_datetime_format=True)\n",
    "# infer_datetime_format=True : usar sólo cuando sea extrictamente necesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dia_mes = datos['nueva_columna'].dt.day  # día del mes\n",
    "\n",
    "# quitar los datos no disponibles  \n",
    "dia_mes  = dia_mes .dropna()\n",
    "\n",
    "# plot the day of the month\n",
    "sns.distplot(dia_mes, kde=False, bins=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_lengths = datos.Date.str.len()  # comprobar la longitud de los valores\n",
    "date_lengths.value_counts()\n",
    "\n",
    "indices = np.where([date_lengths == 24])[1]\n",
    "print('Indices con datos corruptos:', indices)\n",
    "datos.loc[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08ffc6",
   "metadata": {},
   "source": [
    "### 2.6- Entradas de datos inconsistentes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedda76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "professors['Country'] = professors['Country'].str.lower() # convertir a minúsculas\n",
    "professors['Country'] = professors['Country'].str.strip() # quitar espacios en blanco, al principio y fin\n",
    "\n",
    "# coger los 10 más cercanos a south korea\n",
    "countries = professors['Country'].unique()\n",
    "matches = fuzzywuzzy.process.extract(\"south korea\", countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "def replace_matches_in_column(df, column, string_to_match, min_ratio = 47):\n",
    "    # lista de valores únicos\n",
    "    strings = df[column].unique()\n",
    "    \n",
    "    # coger los 10 más cercanos a la cadena de entrada\n",
    "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n",
    "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "    # coger solo los que superan el límite\n",
    "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
    "\n",
    "    # coger las filas que encajan en nuestra tabla\n",
    "    rows_with_matches = df[column].isin(close_matches)\n",
    "\n",
    "    # reemplace todas las filas con coincidencias cercanas con las coincidencias de entrada \n",
    "    df.loc[rows_with_matches, column] = string_to_match\n",
    "    \n",
    "    # avisar cuando acaba la función\n",
    "    print(\"All done!\")\n",
    "    \n",
    "replace_matches_in_column(df=professors, column='Country', string_to_match=\"south korea\")\n",
    "countries = professors['Country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a04439",
   "metadata": {},
   "source": [
    "### 2.7- Ingeniería de datos\n",
    "Modificar los datos para mejorar la resolución del problema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25df58bc",
   "metadata": {},
   "source": [
    "Ventajas:\n",
    "- Mejora del rendimiento en las predicciones\n",
    "- Disminuye necesidad computacional y de datos\n",
    "- Mejora la interpretabilidad de los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92601c09",
   "metadata": {},
   "source": [
    "#### 2.7.1- Información mutua (Mutual Information, MI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c2a1d1",
   "metadata": {},
   "source": [
    "Primero crear una métrica que mida la utilidad de cada catracterítica con el objetivo, para luego escoger un conjunto menor con las características mas útiles.\n",
    "Para ello podemos usar la métrica **Información mutua** que encuentra relaciones de cualquier tipo entre variables, no sólo lineales como el coeficiente de correlación. \n",
    "Mide el grado que una variable reduce la incertidumbre sobre el objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14e1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(X, y, discrete_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02974def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n",
    "\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 5))\n",
    "plot_mi_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48113ecb",
   "metadata": {},
   "source": [
    "#### 2.7.2- Creando nuevas características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d76d7",
   "metadata": {},
   "source": [
    "Es importante:\n",
    "- estudiar bien:\n",
    " - las características que tenemos\n",
    " - el dominio de conocimiento del problema\n",
    " - trabajos previos\n",
    "- visualizar los datos\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3084ed1d",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "- Los modelos lineales sólo aprenden sumas y diferencias de forma natural, pero no pueden aprender nada más complejo.\n",
    "- Los modelos no suelen aprender ratios. Si se usan mejora el rendimiento.\n",
    "- Modelos lineales y redes neuronales -> normalizar primero\n",
    "- Los conteos son especialmente útiles en los árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97b171",
   "metadata": {},
   "source": [
    "##### 2.7.2.1- Transformaciones matemáticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f30ed9",
   "metadata": {},
   "source": [
    "Ratios entre variables y totales\n",
    "Cambio de rango y de forma: normalización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a536b",
   "metadata": {},
   "source": [
    "##### 2.7.2.2-  Conteos\n",
    "Pasando a binario las variables (0 - ausencia, 1- presencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddda5dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "roadway_features = [\"Amenity\", \"Bump\", \"Crossing\", \"GiveWay\",\n",
    "    \"Junction\", \"NoExit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\",\n",
    "    \"TrafficCalming\", \"TrafficSignal\"]\n",
    "\n",
    "# se crea una nueva variable con el conteo\n",
    "accidents[\"RoadwayFeatures\"] = accidents[roadway_features].sum(axis=1)\n",
    "# se muestran los 10 primeros casos\n",
    "accidents[roadway_features + [\"RoadwayFeatures\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bf713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [ \"Cement\", \"BlastFurnaceSlag\", \"FlyAsh\", \"Water\",\n",
    "               \"Superplasticizer\", \"CoarseAggregate\", \"FineAggregate\"]\n",
    "# se crea una nueva variable con el conteo cuando es mayor que 0 (gt(0))\n",
    "concrete[\"Components\"] = concrete[components].gt(0).sum(axis=1)  \n",
    "# se muestran los 10 primeros casos\n",
    "concrete[components + [\"Components\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbddf97",
   "metadata": {},
   "source": [
    "##### 2.7.2.3-  Nuevas variables a partir de unión o separación de otras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d017b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer[[\"Type\", \"Level\"]] = (  # Crear dos nueva variables\n",
    "    customer[\"ColumnaA\"]         # de la variable ColumnaA\n",
    "    .str                         # \n",
    "    .split(\" \", expand=True)     # separando por \" \"\n",
    "                                 # y expandiendo en columna separadas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23389f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos[\"variable A+B\"] = datos[\"variable A\"] + \"_\" + datos[\"variable B\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c944e2e2",
   "metadata": {},
   "source": [
    "##### 2.7.2.4-  Transformaciones de grupos\n",
    "Agregando información entre diferentes filas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fdac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer[\"AverageIncome\"] = (\n",
    "    customer.groupby(\"State\")  # por cada 'State'\n",
    "    [\"Income\"]                 # seleccionar 'Income'\n",
    "    .transform(\"mean\")         # y calcula la media (max, min, median, var, std, and count)\n",
    ")\n",
    "\n",
    "customer[[\"State\", \"Income\", \"AverageIncome\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8479cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer[\"StateFreq\"] = (      # para calcular la frecuencia\n",
    "    customer.groupby(\"State\")\n",
    "    [\"State\"]\n",
    "    .transform(\"count\")\n",
    "    / customer.State.count()\n",
    ")\n",
    "\n",
    "customer[[\"State\", \"StateFreq\"]].head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1b52e",
   "metadata": {},
   "source": [
    "### Separar los datos para entrenar y validar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b38652",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_full.select_dtypes(exclude=['object']) # To keep things simple, we'll use only numerical predictors\n",
    "X_test = X_test_full.select_dtypes(exclude=['object'])\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7499ce9e",
   "metadata": {},
   "source": [
    "## 3- Seleccionar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor  # RandomForestRegressor\n",
    "\n",
    "modelo = DecisionTreeRegressor(random_state=1)    # escogemos el modelo Árbol de decisión\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d28642a",
   "metadata": {},
   "source": [
    "## 4- Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b668efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.fit(train_X, train_y)                      # se entrena el modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a5e1e",
   "metadata": {},
   "source": [
    "## 5- Evaluar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predicciones = modelo.predict(val_X)          # se hace predicción\n",
    "mean_absolute_error(val_y, val_predicciones)      # se evalua el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4222c411",
   "metadata": {},
   "source": [
    "### Underfitting and Overfitting\n",
    "\n",
    "   * *Underfitting*: no encuentra bien patrones relevantes, no predice bien\n",
    "   * *Overfitting*:  se ajusta demasiado a los datos de entrenamiento, no generaliza bien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34286ae4",
   "metadata": {},
   "source": [
    "## 6- Ajustar los parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d354d039",
   "metadata": {},
   "source": [
    "## 7- Generar la predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca17af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X_test = pd.DataFrame(final_imputer.transform(X_test))   # usar total de datos transformados\n",
    "\n",
    "# Sacar las predicciones con los datos para test \n",
    "preds_test = model.predict(final_X_test)\n",
    "\n",
    "\n",
    "# Guardar datos a archivo\n",
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'SalePrice': preds_test})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95601303",
   "metadata": {},
   "source": [
    "## Otros puntos a tener en cuenta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dda025",
   "metadata": {},
   "source": [
    "### Pipelines (canalizaciones)\n",
    "Para agrupar partes de código. Ventajas:\n",
    "- Código limpio\n",
    "- Menos errores\n",
    "- Más fácil de llevar a producción\n",
    "- Más opciones para validar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593bbcb8",
   "metadata": {},
   "source": [
    "#### 1- Definir pasos del preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bf1c6cc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numerical_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3961/2701167841.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m preprocessor = ColumnTransformer(\n\u001b[1;32m     17\u001b[0m     transformers=[\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m'num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumerical_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumerical_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numerical_cols' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Preprocesamiento de datos numéricos \n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocesamiento de datos categóricos\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Paquete de prepocesado\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5dfc9e",
   "metadata": {},
   "source": [
    "#### 2- Definir modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85346114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12749929",
   "metadata": {},
   "source": [
    "#### 3- Crear y evaluar el preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c5cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Agrupar código de preprocesamiento y modelado en una canalización \n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Procesamiento previo de datos de entrenamiento, modelo de ajuste\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Procesamiento previo de datos de validación, obtención de predicciones\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluar el modelo\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2413ca00",
   "metadata": {},
   "source": [
    "### Validación cruzada (Cross-Validation)\n",
    "Divide los datos en subconjuntos, realiza el entrenamiento y validación alternando los subconjuntos y calcula la media al final.\n",
    "Para conjuntos de datos grandes es posible que no sea necesario. Si Cross-validation da resultados similares en cada subconjunto no es necesario hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n",
    "                              ('model', RandomForestRegressor(n_estimators=50,\n",
    "                                                              random_state=0))\n",
    "                             ])\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"MAE scores:\\n\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1c2d8",
   "metadata": {},
   "source": [
    "### Método XGBoost\n",
    "Consiste en un ciclo donde se van añadiendo modelos reduciéndose el error paulatinamente.\n",
    "<br> Usar con datos tabulados, no imagenes o video..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307cf7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "my_model = XGBRegressor()\n",
    "my_model.fit(X_train, y_train)\n",
    "\n",
    "predictions = my_model.predict(X_valid)\n",
    "print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0469022d",
   "metadata": {},
   "source": [
    "**n_estimators** = nº ciclos = nº modelos a añadir. Suele  estar entre 100-1000 (depende mucho del ratio de aprendizaje).\n",
    "<br>\n",
    "- muy bajo -> underfitting\n",
    "- muy alto -> overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4059a03",
   "metadata": {},
   "source": [
    "**early_stopping_rounds** encuentra automáticamente el valor óptimo de 'n_estimators'. \n",
    "Para el entrenamiento cuando el valor de validación deja de mejorar. Valor adecuado=5, para cuando lleva 5 ciclos empeorando la validación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0237b6",
   "metadata": {},
   "source": [
    "**learning_rate** = ratio de aprendizaje. Por defecto 0.1.  \n",
    "Por lo general, es mejor nº alto de 'n_estimators' y  bajo de 'learning_rate'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81071e12",
   "metadata": {},
   "source": [
    "**n_jobs** = para ejecución en paralelo de grandes conjuntos de datos = nº nucleos del ordenador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)], \n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a19e770",
   "metadata": {},
   "source": [
    "### Data leakage (fuga de datos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197c983",
   "metadata": {},
   "source": [
    "Ocurre cuando el conjunto de datos tiene información de la variable de destino.\n",
    "Produce alta precisión en el entrenamiento, pero muy baja en la predicciones reales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338ffa05",
   "metadata": {},
   "source": [
    "Tipos:\n",
    "- **target leakage**: cuando se incluyen datos que tienen información posterior a lo que se desea predecir\n",
    "- **train-test contamination**: ocurre cuando se mezclan los datos de entrenamiento y los de validación a la hora de entrenar el modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5284f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
