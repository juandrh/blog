{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42599432",
   "metadata": {},
   "source": [
    "# \"1.4- NLP: Traducción automática y búsqueda de documentos\"\n",
    "> \"Procesamiento del lenguaje natural con Clasificación y Espacios Vectoriales\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [python, NLP, ML]\n",
    "- image: images/framework.jpg\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0148a1e2",
   "metadata": {},
   "source": [
    "Fuentes:\n",
    "[DeepLearning.AI: Procesamiento de lenguaje natural. Programa especializado en Coursera](https://www.coursera.org/specializations/natural-language-processing) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb0bdf3-c7e5-40db-bcf6-9e9d6203e3fd",
   "metadata": {},
   "source": [
    "> tip: Puedes ver este post en GitHub o ejecutarlo en Binder o Google Colab, pulsa el icono."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b95f76",
   "metadata": {},
   "source": [
    "# Transformar vectores de palabras\n",
    "- Del lenguaje X to languaje Y -> X R =Y\n",
    "- Aprender R:\n",
    "- ![](images/nlp027.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b08cc26-c71d-4d6f-bfff-e328b5700fb1",
   "metadata": {},
   "source": [
    "![](images/nlp028.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e0c654-09e1-49b4-a86e-9d14284d18e2",
   "metadata": {},
   "source": [
    "# K-vecinos más próximos (KNN: K-nearest neighbors)\n",
    "Necesitamos encontrar los vectores más similares a uno dado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3fb39b-02f5-4040-8af2-edf26b265360",
   "metadata": {},
   "source": [
    "## Hashing\n",
    "- **[hash_function](https://es.wikipedia.org/wiki/Funci%C3%B3n_hash)** = toma datos de tamaños arbitrarios y los asigna a un valor fijo.\n",
    "- No hace falta compararlo con todos los ejemplos, simplemente con los valores en el mismo hash_bucket (cubo hash) al que se ha aplicado a la entrada el hash.\n",
    "\n",
    "[+info](https://es.wikipedia.org/wiki/K_vecinos_m%C3%A1s_pr%C3%B3ximos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e99a1e-5963-44a6-a04d-b6c42536b6f6",
   "metadata": {},
   "source": [
    "![](images/nlp029.png)\n",
    "![](images/nlp030.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6366d5d-f2cc-45fa-a3d7-814d6bf35e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_hash_table(value_l, n_buckets):\n",
    "    \n",
    "    def hash_function(value, n_buckets):\n",
    "        return int(value) % n_buckets\n",
    "    \n",
    "    hash_table = {i:[] for i in range(n_buckets)} # Initialize all the buckets in the hash table as empty lists\n",
    "\n",
    "    for value in value_l:\n",
    "        hash_value = hash_function(value,n_buckets) # Get the hash key for the given value\n",
    "        hash_table[hash_value].append(value) # Add the element to the corresponding bucket\n",
    "    \n",
    "    return hash_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edf20c6b-7050-4c03-84e2-d63b26dc0b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   0: [100, 10],\n",
      "    1: [],\n",
      "    2: [],\n",
      "    3: [],\n",
      "    4: [14],\n",
      "    5: [],\n",
      "    6: [],\n",
      "    7: [17, 97],\n",
      "    8: [],\n",
      "    9: []}\n"
     ]
    }
   ],
   "source": [
    "value_l = [100, 10, 14, 17, 97] # Set of values to hash\n",
    "hash_table_example = basic_hash_table(value_l, n_buckets=10)\n",
    "pp.pprint(hash_table_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cabcc2-f064-4549-a951-932972482f16",
   "metadata": {},
   "source": [
    "## hashing sensible a la localización\n",
    "técnica que le permite hacer hash a entradas similares en los mismos cubos con alta probabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed546e-cf1e-41e7-b2b5-5989ab608b60",
   "metadata": {},
   "source": [
    "![](images/nlp031.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b7f36-90e9-44b1-ba2e-4a8a495d1ad8",
   "metadata": {},
   "source": [
    "## Múltiples planos\n",
    "para conseguir un único valor hash."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592eb044-9f32-429a-87a6-cc5d4f8fdc27",
   "metadata": {},
   "source": [
    "![](images/nlp032.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a2486fcc-cb2f-451e-9232-e1cca83e6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   # library for array and matrix manipulation\n",
    "import pandas as pd\n",
    "import pprint                     # utilities for console printing \n",
    "import matplotlib.pyplot as plt   # visualization library\n",
    "pp = pprint.PrettyPrinter(indent=4) # Instantiate a pretty printer\n",
    "import pdb\n",
    "import pickle\n",
    "import string\n",
    "import time\n",
    "#import gensim\n",
    "import nltk\n",
    "import scipy\n",
    "import sklearn\n",
    "import re\n",
    "import string\n",
    "#from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7c76cae1-708b-4c72-8442-3175b4811214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n",
    "filePath = f\"{getcwd()}/tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b96a901-e328-46f0-a091-0c142e11c8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(file_name):\n",
    "    \"\"\"\n",
    "    This function returns the english to french dictionary given a file where the each column corresponds to a word.\n",
    "    Check out the files this function takes in your workspace.\n",
    "    \"\"\"\n",
    "    my_file = pd.read_csv(file_name, delimiter=' ')\n",
    "    etof = {}  # the english to french dictionary to be returned\n",
    "    for i in range(len(my_file)):\n",
    "        # indexing into the rows.\n",
    "        en = my_file.loc[i][0]\n",
    "        fr = my_file.loc[i][1]\n",
    "        etof[en] = fr\n",
    "\n",
    "    return etof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dde2873c-d460-4363-bd47-f7fa8ed59ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    '''\n",
    "    Input:\n",
    "        A: a numpy array which corresponds to a word vector\n",
    "        B: A numpy array which corresponds to a word vector\n",
    "    Output:\n",
    "        cos: numerical number representing the cosine similarity between A and B.\n",
    "    '''\n",
    "    # you have to set this variable to the true label.\n",
    "    cos = -10\n",
    "    dot = np.dot(A, B)\n",
    "    norma = np.linalg.norm(A)\n",
    "    normb = np.linalg.norm(B)\n",
    "    cos = dot / (norma * normb)\n",
    "\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "844dd36d-6c93-4758-a0c9-e8c0e698ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58473478-6632-4a1e-8df4-81fb4f98a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure to plot and arrows that represents vectors with pyplot\n",
    "def plot_vectors(vectors, colors=['k', 'b', 'r', 'm', 'c'], axes=None, fname='image.svg', ax=None):\n",
    "    scale = 1\n",
    "    scale_units = 'x'\n",
    "    x_dir = []\n",
    "    y_dir = []\n",
    "    \n",
    "    for i, vec in enumerate(vectors):\n",
    "        x_dir.append(vec[0][0])\n",
    "        y_dir.append(vec[0][1])\n",
    "    \n",
    "    if ax == None:\n",
    "        fig, ax2 = plt.subplots()\n",
    "    else:\n",
    "        ax2 = ax\n",
    "      \n",
    "    if axes == None:\n",
    "        x_axis = 2 + np.max(np.abs(x_dir))\n",
    "        y_axis = 2 + np.max(np.abs(y_dir))\n",
    "    else:\n",
    "        x_axis = axes[0]\n",
    "        y_axis = axes[1]\n",
    "        \n",
    "    ax2.axis([-x_axis, x_axis, -y_axis, y_axis])\n",
    "        \n",
    "    for i, vec in enumerate(vectors):\n",
    "        ax2.arrow(0, 0, vec[0][0], vec[0][1], head_width=0.05 * x_axis, head_length=0.05 * y_axis, fc=colors[i], ec=colors[i])\n",
    "    \n",
    "    if ax == None:\n",
    "        plt.show()\n",
    "        fig.savefig(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29b60cd8-dbef-4c5d-885e-49a12c78462b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "P = np.array([[1, 1.5]])  # Define a single plane. You may change the direction\n",
    "\n",
    "# Get a new plane perpendicular to P. We use a rotation matrix\n",
    "PT = np.dot([[0, 1], [-1, 0]], P.T).T  \n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 8)) # Create a plot with custom size\n",
    "\n",
    "plot_vectors([P], colors=['b'], axes=[2, 2], ax=ax1) # Plot the plane P as a vector\n",
    "\n",
    "# Plot the plane P as a 2 vectors. \n",
    "# We scale by 2 just to get the arrows outside the current box\n",
    "plot_vectors([PT * 4, PT * -4], colors=['k', 'k'], axes=[4, 4], ax=ax1)\n",
    "\n",
    "# Plot 20 random points. \n",
    "for i in range(0, 20):\n",
    "        v1 = np.array(np.random.uniform(-4, 4, 2)) # Get a pair of random numbers between -4 and 4 \n",
    "        side_of_plane = np.sign(np.dot(P, v1.T)) # Get the sign of the dot product with P\n",
    "        # Color the points depending on the sign of the result of np.dot(P, point.T)\n",
    "        if side_of_plane == 1:\n",
    "            ax1.plot([v1[0]], [v1[1]], 'bo') # Plot a blue point\n",
    "        else:\n",
    "            ax1.plot([v1[0]], [v1[1]], 'ro') # Plot a red point\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0944bc4d-6e91-429f-9c75-b4ff45d98e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def side_of_plane_scalar(P, v):  # scalar\n",
    "    dotproduct = np.dot(P, v.T) # Get the dot product P * v'\n",
    "    sign_of_dot_product = np.sign(dotproduct) # The sign of the elements of the dotproduct matrix \n",
    "    sign_of_dot_product_scalar = sign_of_dot_product.item() # The value of the first item\n",
    "    return sign_of_dot_product_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15e1f298-2928-47c2-ab9a-cda14d743704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_multi_plane(P_l, v):\n",
    "    hash_value = 0\n",
    "    for i, P in enumerate(P_l):\n",
    "        sign = side_of_plane_scalar(P,v)\n",
    "        hash_i = 1 if sign >=0 else 0\n",
    "        hash_value += 2**i * hash_i\n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae0340bf-e403-4069-83cb-4e4ca3cff2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = np.array([[1, 1]])   # First plane 2D\n",
    "P2 = np.array([[-1, 1]])  # Second plane 2D\n",
    "P3 = np.array([[-1, -1]]) # Third plane 2D\n",
    "P_l = [P1, P2, P3]  # List of arrays. It is the multi plane\n",
    "\n",
    "# Vector to search\n",
    "v = np.array([[2,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "866d0550-5f15-409e-a2d0-1ed265a4f513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_multi_plane(P_l, v) # Find the number of the plane that containes this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50d56932-89e8-4619-9462-72e062fcf55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side of the plane function. The result is a matrix\n",
    "def side_of_plane_matrix(P, v):\n",
    "    dotproduct = np.dot(P, v.T)\n",
    "    sign_of_dot_product = np.sign(dotproduct) # Get a boolean value telling if the value in the cell is positive or negative\n",
    "    return sign_of_dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b35f05-ab60-4a43-8c50-c55472d3c0f9",
   "metadata": {},
   "source": [
    "## Planos aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e7d54cf8-08df-4ef9-9453-1b7bc44eb46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.62434536 -0.61175641]\n",
      " [-0.52817175 -1.07296862]\n",
      " [ 0.86540763 -2.3015387 ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "num_dimensions = 2 # is 300 in assignment\n",
    "num_planes = 3 # is 10 in assignment\n",
    "random_planes_matrix = np.random.normal(\n",
    "                       size=(num_planes,\n",
    "                             num_dimensions))\n",
    "print(random_planes_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "965de82a-def8-49d7-850a-0b29107b8b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [-1.],\n",
       "       [-1.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sides_l = side_of_plane_matrix(random_planes_matrix, v)\n",
    "sides_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d091f13d-9492-4fbe-958f-851094dd7096",
   "metadata": {},
   "source": [
    "# Vecinos cercanos aproximados\n",
    "Busca equilibrio entre precisión y eficiencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412006b0-567e-404b-bdfa-2e6e19c2d1da",
   "metadata": {},
   "source": [
    "# Traducción automática ingenua"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e82564-8c1a-41bd-9837-83f6227f4b26",
   "metadata": {},
   "source": [
    "## Los word embeddings para palabras en inglés y francés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7284b173-e3da-4c07-9608-ad8916cd0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"./data/en_embeddings.p\", \"rb\"))\n",
    "fr_embeddings_subset = pickle.load(open(\"./data/fr_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "deead068-2339-4bb9-ae62-31fcb6da8410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the English to French training dictionary is 5000\n",
      "The length of the English to French test dictionary is 1500\n"
     ]
    }
   ],
   "source": [
    "# loading the english to french dictionaries\n",
    "en_fr_train = get_dict('./data/en-fr.train.txt')\n",
    "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict('./data/en-fr.test.txt')\n",
    "print('The length of the English to French test dictionary is', len(en_fr_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec5bed7-f3ab-4e34-8f25-44d3c9950b76",
   "metadata": {},
   "source": [
    "### Generar embedding y matrices de transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fc74f2b0-a09c-450d-85ef-4acb5e079bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        en_fr: English to French dictionary\n",
    "        french_vecs: French words to their corresponding word embeddings.\n",
    "        english_vecs: English words to their corresponding word embeddings.\n",
    "    Output: \n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
    "    \"\"\"    \n",
    "    # X_l and Y_l are lists of the english and french word embeddings\n",
    "    X_l = list()\n",
    "    Y_l = list()    \n",
    "    # get the english words (the keys in the dictionary) and store in a set()\n",
    "    english_set = set(english_vecs.keys())  \n",
    "    # get the french words (keys in the dictionary) and store in a set()\n",
    "    french_set = set(french_vecs.keys())    \n",
    "    # store the french words that are part of the english-french dictionary (these are the values of the dictionary)\n",
    "    french_words = set(en_fr.values())\n",
    "    \n",
    "    # loop through all english, french word pairs in the english french dictionary\n",
    "    for en_word, fr_word in en_fr.items():\n",
    "        # check that the french word has an embedding and that the english word has an embedding\n",
    "        if fr_word in french_set and en_word in english_set:\n",
    "            # get the english embedding\n",
    "            en_vec = english_vecs[en_word]\n",
    "            # get the french embedding\n",
    "            fr_vec = french_vecs[fr_word]\n",
    "            # add the english embedding to the list\n",
    "            X_l.append(en_vec)\n",
    "            # add the french embedding to the list\n",
    "            Y_l.append(fr_vec)\n",
    "    # stack the vectors of X_l into a matrix X\n",
    "    X = np.array(X_l)\n",
    "    # stack the vectors of Y_l into a matrix Y\n",
    "    Y = np.array(Y_l) \n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1f4a1a11-9285-490f-be59-84108c310b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the training set:\n",
    "X_train, Y_train = get_matrices(\n",
    "    en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2a154a-f4a7-4009-898d-949530713d0f",
   "metadata": {},
   "source": [
    "### Traducciones\n",
    "#### Traduccion como transformación lineal de embeddings (R)\n",
    "Encontrar una matriz R que minimice la siguiente ecuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ca16aa-5c89-4b7b-bd4e-a1e9c6ca78a6",
   "metadata": {},
   "source": [
    "![](images/nlp033.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca2605bc-e013-4e0a-af7d-d4779b242aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
    "    '''\n",
    "    # m is the number of rows in X\n",
    "    m = len(X)   \n",
    "    # diff is XR - Y    \n",
    "    diff = np.dot(X,R)-Y\n",
    "\n",
    "    # diff_squared is the element-wise square of the difference    \n",
    "    diff_squared = diff**2\n",
    "    # sum_diff_squared is the sum of the squared elements\n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "\n",
    "    # loss i the sum_diff_squard divided by the number of examples (m)\n",
    "    loss = sum_diff_squared / m\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "077c6489-a68a-4aaf-b10d-441d61e6a53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected loss for an experiment with random matrices: 8.1866\n"
     ]
    }
   ],
   "source": [
    "# Testing your implementation.\n",
    "np.random.seed(123)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * .1\n",
    "R = np.random.rand(n, n)\n",
    "print(f\"Expected loss for an experiment with random matrices: {compute_loss(X, Y, R):.4f}\" ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31875eae-bc1c-49db-b001-e0cfc26bd724",
   "metadata": {},
   "source": [
    "### Cálculo del gradiente de la función de coste con respecto a la matriz de transformación R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bc6657-8e7d-46ca-803b-1da9477e5526",
   "metadata": {},
   "source": [
    "![](images/nlp034.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3285b1fe-24d0-421b-a106-4d4d2626ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        g: a scalar value - gradient of the loss function L for given X, Y and R.\n",
    "    '''   \n",
    "    # m is the number of rows in X\n",
    "    m = len(X)\n",
    "\n",
    "    # gradient is X^T(XR - Y) * 2/m    \n",
    "    gradient = np.dot(X.T,(np.dot(X,R)-Y))*2/m\n",
    "\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d5dd037c-f469-4ea0-9601-410d996aa75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of the gradient matrix: [1.3498175  1.11264981 0.69626762 0.98468499 1.33828969]\n"
     ]
    }
   ],
   "source": [
    "# Testing your implementation.\n",
    "np.random.seed(123)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * .1\n",
    "R = np.random.rand(n, n)\n",
    "gradient = compute_gradient(X, Y, R)\n",
    "print(f\"First row of the gradient matrix: {gradient[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc5b561-d0f3-47a7-b3fb-1a133b1c9e32",
   "metadata": {},
   "source": [
    "### Encontrar la R óptima con el algoritmo de descenso de gradiente\n",
    "- Calcular el gradiente g de la función de coste con respecto a la matriz R .\n",
    "- Actualice R con la fórmula: Rnew=Rold−αg , donde α es la tasa de aprendizaje, que es un escalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "23283bbe-f4f9-4250-954d-29c548ed20c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003, verbose=True, compute_loss=compute_loss, compute_gradient=compute_gradient):\n",
    "    '''\n",
    "    Inputs:\n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
    "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
    "    Outputs:\n",
    "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
    "    '''\n",
    "    np.random.seed(129)\n",
    "\n",
    "    # the number of columns in X is the number of dimensions for a word vector (e.g. 300)\n",
    "    # R is a square matrix with length equal to the number of dimensions in th  word embedding\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        if verbose and i % 25 == 0:\n",
    "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "\n",
    "        # use the function that you defined to compute the gradient\n",
    "        gradient = compute_gradient(X, Y, R)\n",
    "\n",
    "        # update R by subtracting the learning rate times gradient\n",
    "        R -= learning_rate * gradient\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "896c9f78-8eac-45bd-a914-3809e696b80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 3.7242\n",
      "loss at iteration 25 is: 3.6283\n",
      "loss at iteration 50 is: 3.5350\n",
      "loss at iteration 75 is: 3.4442\n"
     ]
    }
   ],
   "source": [
    "# Testing your implementation.\n",
    "np.random.seed(129)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * .1\n",
    "R = align_embeddings(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dd446e-a85b-4056-9ea6-40e499cb0235",
   "metadata": {},
   "source": [
    "### Calcular la matriz de transformación R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3a96fad6-f6c8-4acb-b035-3d75ee663228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 963.0146\n",
      "loss at iteration 25 is: 97.8292\n",
      "loss at iteration 50 is: 26.8329\n",
      "loss at iteration 75 is: 9.7893\n",
      "loss at iteration 100 is: 4.3776\n",
      "loss at iteration 125 is: 2.3281\n",
      "loss at iteration 150 is: 1.4480\n",
      "loss at iteration 175 is: 1.0338\n",
      "loss at iteration 200 is: 0.8251\n",
      "loss at iteration 225 is: 0.7145\n",
      "loss at iteration 250 is: 0.6534\n",
      "loss at iteration 275 is: 0.6185\n",
      "loss at iteration 300 is: 0.5981\n",
      "loss at iteration 325 is: 0.5858\n",
      "loss at iteration 350 is: 0.5782\n",
      "loss at iteration 375 is: 0.5735\n"
     ]
    }
   ],
   "source": [
    "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc036312-3eb5-4e23-b3a9-bae6f6c6fc08",
   "metadata": {},
   "source": [
    "### Probando la traducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09583b-b29d-4404-a06c-27594b945ecd",
   "metadata": {},
   "source": [
    "#### Algoritmo k-vecinos más cercanos\n",
    "- k-NN es un método que toma un vector como entrada y encuentra los otros vectores en el conjunto de datos que están más cerca de él.\n",
    "- La 'k' es el número de \"vecinos más cercanos\" a encontrar (por ejemplo, k=2 encuentra los dos vecinos más cercanos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832a64f-f135-4f05-bdfc-150e5ff238ee",
   "metadata": {},
   "source": [
    "#### Buscando la traducción del embedding\n",
    "Dado que estamos aproximando la función de traducción de embeddings de inglés a francés mediante una matriz de transformación lineal R , la mayoría de las veces no obtendremos el embedding exacto de una palabra en francés.\n",
    "\n",
    "Aquí es donde k-NN se vuelve realmente útil. Al usar 1-NN con eR como entrada, podemos buscar un  embedding f (como una fila) en la matriz Y que sea la más cercana al vector transformado eR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "05b70b46-984e-4998-b3d4-575767eed68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1, cosine_similarity=cosine_similarity):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - v, the vector you are going find the nearest neighbor for\n",
    "      - candidates: a set of vectors where we will find the neighbors\n",
    "      - k: top k nearest neighbors to find\n",
    "    Output:\n",
    "      - k_idx: the indices of the top k closest vectors in sorted form\n",
    "    \"\"\"\n",
    "    similarity_l = []\n",
    "\n",
    "    # for each candidate vector...\n",
    "    for row in candidates:\n",
    "        # get the cosine similarity\n",
    "        cos_similarity = np.dot(v,row)/(np.linalg.norm(v)*np.linalg.norm(row))\n",
    "        \n",
    "        # append the similarity to the list\n",
    "        similarity_l.append(cos_similarity)\n",
    "\n",
    "    # sort the similarity list and get the indices of the sorted list    \n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "    \n",
    "    # Reverse the order of the sorted_ids array\n",
    "    sorted_ids = sorted_ids[::-1]\n",
    "    \n",
    "    # get the indices of the k most similar candidate vectors\n",
    "    k_idx = sorted_ids[:k]\n",
    "    ### END CODE HERE ###\n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ae0a44e4-03f1-40bd-b4a2-ca647e2a5073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 1]\n",
      " [1 0 5]\n",
      " [9 9 9]]\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation:\n",
    "v = np.array([1, 0, 1])\n",
    "candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n",
    "print(candidates[nearest_neighbor(v, candidates, 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d4a563-b6d6-4a4c-8aec-c10a3a334d75",
   "metadata": {},
   "source": [
    "#### Prueba la traducción y calcula su precisión\n",
    "Precisión = total correctos / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c55edb78-e265-4a4f-974a-ef4c64fd1eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, Y, R, nearest_neighbor=nearest_neighbor):\n",
    "    '''\n",
    "    Input:\n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the transform matrix which translates word embeddings from\n",
    "        English to French word vector space.\n",
    "    Output:\n",
    "        accuracy: for the English to French capitals\n",
    "    '''\n",
    "    # The prediction is X times R\n",
    "    pred = np.dot(X,R)    \n",
    "    # initialize the number correct to zero\n",
    "    num_correct = 0\n",
    "    # loop through each row in pred (each transformed embedding)\n",
    "    for i in range(len(pred)):\n",
    "        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n",
    "        pred_idx = nearest_neighbor(pred[i], Y, k=1, cosine_similarity=cosine_similarity)\n",
    "\n",
    "        # if the index of the nearest neighbor equals the row of i... \\\n",
    "        if pred_idx == i:\n",
    "            # increment the number correct by 1.\n",
    "            num_correct += 1\n",
    "\n",
    "    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n",
    "    accuracy = num_correct / len(pred)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dc0f8209-0d74-49fc-8d39-cadaf3c89b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is 0.557\n"
     ]
    }
   ],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)\n",
    "acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\n",
    "print(f\"accuracy on test set is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150e19b-65e5-4a3d-8852-064532283596",
   "metadata": {},
   "source": [
    "# LSH (locality sensitive hashing) y búsqueda de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0a438542-1e01-4d5d-911b-9602f90b78a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = all_positive_tweets + all_negative_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd016b8-17bd-4465-b92e-06ba1fa858b0",
   "metadata": {},
   "source": [
    "## Obtener los iembeddings de documentos\n",
    "**Modelos de documento de bolsa de palabras (BOW)**: los documentos de texto son secuencias de palabras.\n",
    "\n",
    "- El orden de las palabras marca la diferencia. Por ejemplo, las oraciones \"La tarta de manzana es mejor que la pizza de pepperoni\". y \"La pizza de pepperoni es mejor que la tarta de manzana\" tienen significados opuestos debido al orden de las palabras.\n",
    "- Sin embargo, para algunas aplicaciones, ignorar el orden de las palabras puede permitirnos entrenar un modelo eficiente y efectivo. Este enfoque se denomina modelo de documento de bolsa de palabras.\n",
    "**Embeddings de documentos**\n",
    "- Los embeddings de documentos se crea sumando los embeddings de todas las palabras en el documento.\n",
    "- Si no conocemos el embeddings de alguna palabra, podemos ignorar esa palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2e3ba664-2a2c-41ce-9f65-7ed0c2a7f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embedding(tweet, en_embeddings, process_tweet=process_tweet):\n",
    "    '''\n",
    "    Input:\n",
    "        - tweet: a string\n",
    "        - en_embeddings: a dictionary of word embeddings\n",
    "    Output:\n",
    "        - doc_embedding: sum of all word embeddings in the tweet\n",
    "    '''\n",
    "    doc_embedding = np.zeros(300)   \n",
    "    # process the document into a list of words (process the tweet)\n",
    "    processed_doc = process_tweet(tweet)\n",
    "    for word in processed_doc:\n",
    "        # add the word embedding to the running total for the document embedding        \n",
    "        if word in en_embeddings.keys():\n",
    "            doc_embedding += en_embeddings[word]\n",
    "\n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fe3f4f16-71cd-426b-836c-720746154ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00268555, -0.15378189, -0.55761719, -0.07216644, -0.32263184])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing your function\n",
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "tweet_embedding = get_document_embedding(custom_tweet, en_embeddings_subset)\n",
    "tweet_embedding[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a43bfb-75c5-42ab-a798-b52c5e2cc4ac",
   "metadata": {},
   "source": [
    "## Almacene todos los vectores de documentos en un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7ddc20f0-2048-42b3-bb25-581d936ec1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vecs(all_docs, en_embeddings, get_document_embedding=get_document_embedding):\n",
    "    '''\n",
    "    Input:\n",
    "        - all_docs: list of strings - all tweets in our dataset.\n",
    "        - en_embeddings: dictionary with words as the keys and their embeddings as the values.\n",
    "    Output:\n",
    "        - document_vec_matrix: matrix of tweet embeddings.\n",
    "        - ind2Doc_dict: dictionary with indices of tweets in vecs as keys and their embeddings as the values.\n",
    "    '''\n",
    "    # the dictionary's key is an index (integer) that identifies a specific tweet\n",
    "    # the value is the document embedding for that document\n",
    "    ind2Doc_dict = {}\n",
    "\n",
    "    # this is list that will store the document vectors\n",
    "    document_vec_l = []\n",
    "\n",
    "    for i, doc in enumerate(all_docs):\n",
    "\n",
    "        # get the document embedding of the tweet\n",
    "        doc_embedding = get_document_embedding(doc, en_embeddings, process_tweet=process_tweet)\n",
    "\n",
    "        # save the document embedding into the ind2Tweet dictionary at index i\n",
    "        ind2Doc_dict[i] = doc_embedding\n",
    "\n",
    "        # append the document embedding to the list of document vectors\n",
    "        document_vec_l.append(doc_embedding)\n",
    "\n",
    "    # convert the list of document vectors into a 2D array (each row is a document vector)\n",
    "    document_vec_matrix = np.vstack(document_vec_l)\n",
    "\n",
    "    return document_vec_matrix, ind2Doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bd522604-2029-4190-9ec2-9ed9e1ad4931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary 10000\n",
      "shape of document_vecs (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "document_vecs, ind2Tweet = get_document_vecs(all_tweets, en_embeddings_subset)\n",
    "print(f\"length of dictionary {len(ind2Tweet)}\")\n",
    "print(f\"shape of document_vecs {document_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da9384-1e6b-4a4f-aac2-eb3123e20773",
   "metadata": {},
   "source": [
    "## Buscando los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "11ea613a-76e8-4d13-ad54-252041689a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweet = 'i am sad'\n",
    "process_tweet(my_tweet)\n",
    "tweet_embedding = get_document_embedding(my_tweet, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2442e6a6-054d-402a-9305-7c1959305a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@zoeeylim sad sad sad kid :( it's ok I help you watch the match HAHAHAHAHA\n"
     ]
    }
   ],
   "source": [
    "# this gives you a similar tweet as your input.\n",
    "# this implementation is vectorized...\n",
    "idx = np.argmax(cosine_similarity(document_vecs, tweet_embedding))\n",
    "print(all_tweets[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f915808-4697-4fe2-9f83-750f5481e48b",
   "metadata": {},
   "source": [
    "## Encontrar los tweets más similares con LSH\n",
    "En lugar de mirar los 10.000 vectores, se puede buscar un subconjunto para encontrar sus vecinos más cercanos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a1710909-43c3-4cca-9a0c-ba91e5a04649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors is 10000 and each has 300 dimensions.\n"
     ]
    }
   ],
   "source": [
    "N_VECS = len(all_tweets)       # This many vectors.\n",
    "N_DIMS = len(ind2Tweet[1])     # Vector dimensionality.\n",
    "print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")\n",
    "# The number of planes. We use log2(256) to have ~16 vectors/bucket.\n",
    "N_PLANES = 10\n",
    "# Number of times to repeat the hashing to improve the search.\n",
    "N_UNIVERSES = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e71a7b6-50d6-4e9d-a953-fb8b6429ac09",
   "metadata": {},
   "source": [
    "## Elegir el número de planos\n",
    "- Cada plano divide el espacio en 2 partes.\n",
    "- Entonces n planos dividen el espacio en 2n cubos de hash.\n",
    "- Queremos organizar 10,000 vectores de documentos en cubos para que cada cubo tenga alrededor de 16 vectores.\n",
    "- Para eso necesitamos 10000/16=625 cubos.\n",
    "- Nos interesa n , número de planos, por lo que 2n=625 . Ahora, podemos calcular n=log2 625=9.29≈10 ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4e8691-7842-446a-be46-f322bdbdfce1",
   "metadata": {},
   "source": [
    "## Obtener el número hash de un vector\n",
    "Para cada vector, necesitamos obtener un número único asociado a ese vector para asignarlo a un \"cubo de hash\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3374e7b4-6e62-4c69-8ba8-32d25cbc4131",
   "metadata": {},
   "source": [
    "![](images/nlp035.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f0d127bb-7aa9-4b07-8760-6eedc7ce1852",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "planes_l = [np.random.normal(size=(N_DIMS, N_PLANES))\n",
    "            for _ in range(N_UNIVERSES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3f0ade68-e0b2-496b-adc8-c4b1501a8a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_value_of_vector(v, planes):\n",
    "    \"\"\"Create a hash for a vector; hash_id says which random hash to use.\n",
    "    Input:\n",
    "        - v:  vector of tweet. It's dimension is (1, N_DIMS)\n",
    "        - planes: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n",
    "    Output:\n",
    "        - res: a number which is used as a hash for your vector\n",
    "\n",
    "    \"\"\"\n",
    "    # for the set of planes, calculate the dot product between the vector and the matrix containing the planes\n",
    "    # remember that planes has shape (300, 10) The dot product will have the shape (1,10)    \n",
    "    dot_product = np.dot(v,planes)\n",
    "        \n",
    "    # get the sign of the dot product (1,10) shaped vector\n",
    "    sign_of_dot_product = np.sign(dot_product )\n",
    "      \n",
    "    # set h to be false (eqivalent to 0 when used in operations) if the sign is negative,\n",
    "    # and true (equivalent to 1) if the sign is positive (1,10) shaped vector\n",
    "    # if the sign is 0, i.e. the vector is in the plane, consider the sign to be positive  \n",
    "    \n",
    "    h= sign_of_dot_product >= 1     \n",
    "    # remove extra un-used dimensions (convert this from a 2D to a 1D array)\n",
    "    h = np.squeeze(h)       \n",
    "    # initialize the hash value to 0\n",
    "    hash_value = 0\n",
    "\n",
    "    n_planes = len(planes[0])\n",
    "    for i in range(n_planes):\n",
    "        # increment the hash value by 2^i * h_i        \n",
    "        hash_value += (2**i)* h[i]\n",
    "\n",
    "    # cast hash_value as an integer\n",
    "    hash_value = int(hash_value)\n",
    "\n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0f9f5118-06f2-408c-8af3-f41ea10daf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The hash value for this vector, and the set of planes at index 0, is 768\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "idx = 0\n",
    "planes = planes_l[idx]  # get one 'universe' of planes to test the function\n",
    "vec = np.random.rand(1, 300)\n",
    "print(f\" The hash value for this vector,\",\n",
    "      f\"and the set of planes at index {idx},\",\n",
    "      f\"is {hash_value_of_vector(vec, planes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f780fa-dd49-4349-b257-835aba1df87a",
   "metadata": {},
   "source": [
    "## Crear una tabla hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c94f665f-0bda-4b28-91e4-80d6f482d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_table(vecs, planes, hash_value_of_vector=hash_value_of_vector):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - vecs: list of vectors to be hashed.\n",
    "        - planes: the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n",
    "    Output:\n",
    "        - hash_table: dictionary - keys are hashes, values are lists of vectors (hash buckets)\n",
    "        - id_table: dictionary - keys are hashes, values are list of vectors id's\n",
    "                            (it's used to know which tweet corresponds to the hashed vector)\n",
    "    \"\"\"\n",
    "    # number of planes is the number of columns in the planes matrix\n",
    "    num_of_planes = len(planes[0])\n",
    "\n",
    "    # number of buckets is 2^(number of planes)    \n",
    "    num_buckets = 2**num_of_planes\n",
    "\n",
    "    # create the hash table as a dictionary.\n",
    "    # Keys are integers (0,1,2.. number of buckets)\n",
    "    # Values are empty lists\n",
    "    hash_table = {i : [] for i in range(num_buckets)}\n",
    "     \n",
    "    # create the id table as a dictionary.\n",
    "    # Keys are integers (0,1,2... number of buckets)\n",
    "    # Values are empty lists\n",
    "    id_table = {i : [] for i in range(num_buckets)}  \n",
    "    \n",
    "    # for each vector in 'vecs'\n",
    "    for i, v in enumerate(vecs):           \n",
    "        \n",
    "        # calculate the hash value for the vector\n",
    "        h = hash_value_of_vector(v, planes)\n",
    "       \n",
    "        # store the vector into hash_table at key h,\n",
    "        # by appending the vector v to the list at key h\n",
    "        hash_table[h].append(v)\n",
    "        \n",
    "        # store the vector's index 'i' (each document is given a unique integer 0,1,2...)\n",
    "        # the key is the h, and the 'i' is appended to the list at key h\n",
    "        id_table[h].append(i)\n",
    "\n",
    "    return hash_table, id_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d89c3735-eeb1-4d11-873e-fa1148e8b8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash table at key 0 has 1351 document vectors\n",
      "The id table at key 0 has 1351\n",
      "The first 5 document indices stored at key 0 of are [3, 8, 16, 18, 29]\n"
     ]
    }
   ],
   "source": [
    "planes = planes_l[0]  # get one 'universe' of planes to test the function\n",
    "tmp_hash_table, tmp_id_table = make_hash_table(document_vecs, planes)\n",
    "\n",
    "print(f\"The hash table at key 0 has {len(tmp_hash_table[0])} document vectors\")\n",
    "print(f\"The id table at key 0 has {len(tmp_id_table[0])}\")\n",
    "print(f\"The first 5 document indices stored at key 0 of are {tmp_id_table[0][0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fcd29c-e9b1-496e-81a7-ccbfda9ac710",
   "metadata": {},
   "source": [
    "## Crear todas las tablas hash\n",
    "haga un hash de sus vectores y guárdelos en una tabla hash que le permita buscar rápidamente y buscar vectores similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "890f5bb4-fd5b-4cdf-a80c-ec94be12461e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on hash universe #: 0\n",
      "working on hash universe #: 1\n",
      "working on hash universe #: 2\n",
      "working on hash universe #: 3\n",
      "working on hash universe #: 4\n",
      "working on hash universe #: 5\n",
      "working on hash universe #: 6\n",
      "working on hash universe #: 7\n",
      "working on hash universe #: 8\n",
      "working on hash universe #: 9\n",
      "working on hash universe #: 10\n",
      "working on hash universe #: 11\n",
      "working on hash universe #: 12\n",
      "working on hash universe #: 13\n",
      "working on hash universe #: 14\n",
      "working on hash universe #: 15\n",
      "working on hash universe #: 16\n",
      "working on hash universe #: 17\n",
      "working on hash universe #: 18\n",
      "working on hash universe #: 19\n",
      "working on hash universe #: 20\n",
      "working on hash universe #: 21\n",
      "working on hash universe #: 22\n",
      "working on hash universe #: 23\n",
      "working on hash universe #: 24\n"
     ]
    }
   ],
   "source": [
    "def create_hash_id_tables(n_universes):\n",
    "    hash_tables = []\n",
    "    id_tables = []\n",
    "    for universe_id in range(n_universes):  # there are 25 hashes\n",
    "        print('working on hash universe #:', universe_id)\n",
    "        planes = planes_l[universe_id]\n",
    "        hash_table, id_table = make_hash_table(document_vecs, planes)\n",
    "        hash_tables.append(hash_table)\n",
    "        id_tables.append(id_table)\n",
    "    \n",
    "    return hash_tables, id_tables\n",
    "\n",
    "hash_tables, id_tables = create_hash_id_tables(N_UNIVERSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ecc2d-c8c2-4d45-a742-d7a2190ad120",
   "metadata": {},
   "source": [
    "## K-NN aproximado\n",
    "usando hashing sensible a la localidad, para buscar documentos que sean similares a un documento dado en el índice doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b611d95e-04be-4741-85d6-63dfaefb3ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_knn(doc_id, v, planes_l, hash_tables, id_tables, k=1, num_universes_to_use=25, hash_value_of_vector=hash_value_of_vector):\n",
    "    \"\"\"Search for k-NN using hashes.\"\"\"\n",
    "    #assert num_universes_to_use <= N_UNIVERSES      \n",
    "    vecs_to_consider_l = list()   # Vectors that will be checked as possible nearest neighbor \n",
    "    ids_to_consider_l = list()  # list of document IDs   \n",
    "    ids_to_consider_set = set()  # create a set for ids to consider, for faster checking if a document ID already exists in the set\n",
    "\n",
    "    # loop through the universes of planes\n",
    "    for universe_id in range(num_universes_to_use):\n",
    "        \n",
    "        planes = planes_l[universe_id] # get the set of planes from the planes_l list, for this particular universe_id       \n",
    "        hash_value = hash_value_of_vector(v, planes)  # get the hash value of the vector for this set of planes        \n",
    "        hash_table = hash_tables[universe_id]  # get the hash table for this particular universe_id       \n",
    "        document_vectors_l = hash_table[hash_value]   # get the list of document vectors for this hash table, where the key is the hash_value        \n",
    "        id_table = id_tables[universe_id]  # get the id_table for this particular universe_id        \n",
    "        new_ids_to_consider = id_table[hash_value] # get the subset of documents to consider as nearest neighbors from this id_table dictionary\n",
    "\n",
    "        if doc_id in new_ids_to_consider:\n",
    "            new_ids_to_consider.remove(doc_id)   #-----\n",
    "            print(\n",
    "                f\"removed doc_id {doc_id} of input vector from new_ids_to_search\")\n",
    "\n",
    "        # loop through the subset of document vectors to consider\n",
    "        for i, new_id in enumerate(new_ids_to_consider):\n",
    "\n",
    "            # if the document ID is not yet in the set ids_to_consider...\n",
    "            if new_id not in ids_to_consider_set:\n",
    "                # access document_vectors_l list at index i to get the embedding\n",
    "                # then append it to the list of vectors to consider as possible nearest neighbors\n",
    "                document_vector_at_i = get_document_embedding(all_tweets[i], en_embeddings_subset, process_tweet=process_tweet)        #-----\n",
    "                vecs_to_consider_l.append(document_vector_at_i)                  #-----\n",
    "\n",
    "                # append the new_id (the index for the document) to the list of ids to consider\n",
    "                ids_to_consider_l.append(new_id)\n",
    "\n",
    "                # also add the new_id to the set of ids to consider\n",
    "                # (use this to check if new_id is not already in the IDs to consider)\n",
    "                ids_to_consider_set.add(new_id)\n",
    "\n",
    "    # Now run k-NN on the smaller set of vecs-to-consider.\n",
    "    print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n",
    "\n",
    "    # convert the vecs to consider set to a list, then to a numpy array\n",
    "    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n",
    "\n",
    "    # call nearest neighbors on the reduced list of candidate vectors\n",
    "    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n",
    "\n",
    "    # Use the nearest neighbor index list as indices into the ids to consider\n",
    "    # create a list of nearest neighbors by the document ids\n",
    "    nearest_neighbor_ids = [ids_to_consider_l[idx]\n",
    "                            for idx in nearest_neighbor_idx_l]\n",
    "\n",
    "    return nearest_neighbor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f4b62ab7-11dc-4914-ac66-25af96b08e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_vecs, ind2Tweet\n",
    "doc_id = 5\n",
    "doc_to_search = all_tweets[doc_id]\n",
    "vec_to_search = document_vecs[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8eb516a6-13cc-44a1-bcb4-6750abd04ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed doc_id 5 of input vector from new_ids_to_search\n",
      "removed doc_id 5 of input vector from new_ids_to_search\n",
      "removed doc_id 5 of input vector from new_ids_to_search\n",
      "removed doc_id 5 of input vector from new_ids_to_search\n",
      "removed doc_id 5 of input vector from new_ids_to_search\n",
      "Fast considering 150 vecs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juan_\\AppData\\Local\\Temp/ipykernel_16868/2362656131.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cos_similarity = np.dot(v,row)/(np.linalg.norm(v)*np.linalg.norm(row))\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbor_ids = approximate_knn(\n",
    "    doc_id, vec_to_search, planes_l, hash_tables, id_tables, k=3, num_universes_to_use=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c31147eb-a47e-49d2-b535-149a6d7e43d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for document 5\n",
      "Document contents: @BhaktisBanter @PallaviRuhail This one is irresistible :)\n",
      "#FlipkartFashionFriday http://t.co/EbZ0L2VENM\n",
      "\n",
      "Nearest neighbor at document id 4193\n",
      "document contents: This girl :)  https://t.co/OAXMGGICNr\n",
      "Nearest neighbor at document id 4045\n",
      "document contents: @LemonyLimeUK thanks for the follow have a great day :)\n",
      "Nearest neighbor at document id 8914\n",
      "document contents: @zaynmalik zayn come back to 1D already. :(\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nearest neighbors for document {doc_id}\")\n",
    "print(f\"Document contents: {doc_to_search}\")\n",
    "print(\"\")\n",
    "\n",
    "for neighbor_id in nearest_neighbor_ids:\n",
    "    print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
    "    print(f\"document contents: {all_tweets[neighbor_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d96fe-6991-4b6b-812d-bae94d7f8522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09993e7e-9759-4bd7-be20-23265d874756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
