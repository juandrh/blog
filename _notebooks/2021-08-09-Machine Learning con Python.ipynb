{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7424bccd",
   "metadata": {},
   "source": [
    "# \"Machine learning con Python\"\n",
    "> \"Guía de referencia rápidda de machine learning con Python\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [python, kaggle, ML]\n",
    "- image: images/python.png\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77efb9f",
   "metadata": {},
   "source": [
    "> tip: Puedes ver este post en GitHub o ejecutarlo en Binder o Google Colab, pulsa el icono."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5d3f3d",
   "metadata": {},
   "source": [
    "Fuentes:<br>\n",
    "[Cursos Machine Learning - Kaggle](https://www.kaggle.com/learn)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df811a4",
   "metadata": {},
   "source": [
    "## 1 - Recoger los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac7fbf8",
   "metadata": {},
   "source": [
    "## 2 - Preparar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d829f",
   "metadata": {},
   "source": [
    "### 2.1 - Exploración de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datos = pd.read_csv(ruta_archivo) # cargar datos desde tabla en archivo csv\n",
    "datos.describe()                  # imprime resumen de los datos\n",
    "datos.columns                     # imprime lista de las columnas\n",
    "datos.head()                      # imprime lista de las 5 primeras muestras de la tabla\n",
    "\n",
    "datos_filtrados = datos.dropna(axis=0) # elimina datos na (no available - no disponibles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf7852",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True) # Eliminar filas sin datos destino, \n",
    "\n",
    "# elegir la columna que deseamos estimar en nuestro modelo\n",
    "y = X_full.SalePrice\n",
    "\n",
    "# elegir las columnas que se pasarán al modelo\n",
    "# opción 1: quitar la columna destino\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# opción 2: elegir sólo algunas columnas\n",
    "datos_features = ['Rooms', 'Bathroom','Lattitude', 'Longtitude'] \n",
    "X = datos[datos_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3125f7",
   "metadata": {},
   "source": [
    "### 2.2- Valores que faltan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce36e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_val_count_by_column = (X_train.isnull().sum()) # Número de valores faltantes en cada columna de datos de entrenamiento \n",
    "\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5698d6b",
   "metadata": {},
   "source": [
    "#### 2.2.1 - Eliminar columna\n",
    "Para el caso en falten la mayoría de datos de una variable. Sino se puede perder información importante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_missing = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]       # recopilar columnas con valores que faltan\n",
    "\n",
    "# eliminar columnas con valores que faltan\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)  \n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e2bfc3",
   "metadata": {},
   "source": [
    "#### 2.2.2 - Imputación\n",
    "Rellenar los valores faltantes con otro valor, por ejemplo: la media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e95063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputación\n",
    "my_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train), index=range(1, X_train.shape[0] + 1),\n",
    "                          columns=range(1, X_train.shape[1] + 1))  \n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid), index=range(1, X_valid.shape[0] + 1),\n",
    "                          columns=range(1, X_valid.shape[1] + 1))  \n",
    "\n",
    "# Imputación quitó los nombres de columnas, volver a ponerlos \n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5373b010",
   "metadata": {},
   "source": [
    "#### 2.2.3 - Extensión de Imputación\n",
    "Añadiendo otra columna que informe si faltaba o no el valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017a42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_plus = X_train.copy() # Hacer una copia para evitar cambiar los datos originales (al imputar) \n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Hacer nuevas columnas que indiquen lo que se imputará. \n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "# Imputación\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "# Imputación quitó los nombres de columnas, volver a ponerlos \n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604069bd",
   "metadata": {},
   "source": [
    "### 2.3.- Variables categóricas\n",
    "Toman un número limitado de valores. Necesitan ser preprocesados antes de usarlos en los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804f1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "# para obtener lista de variables categóricas \n",
    "\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dccfbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_label_cols = [col for col in object_cols if   # Columnas que se pueden codificar con etiquetas de forma segura \n",
    "                   set(X_train[col]) == set(X_valid[col])]\n",
    "        \n",
    "# Columnas problemáticas que se eliminarán del conjunto de datos\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "\n",
    "# Aplicar codificador de etiquetas \n",
    "label_encoder = LabelEncoder()\n",
    "for col in good_label_cols:\n",
    "    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "    label_X_valid[col] = label_encoder.transform(X_valid[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols)) \n",
    "# Obtener el número de entradas únicas en cada columna con datos categóricos \n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# Imprimirc el número de entradas únicas por columna, en orden ascendente \n",
    "sorted(d.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7446af",
   "metadata": {},
   "source": [
    "#### 2.3.1- Eliminar columna\n",
    "Sólo es buena opción si la columna no tiene información útil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20deed7e",
   "metadata": {},
   "source": [
    "#### 2.3.2- Codificación ordinal\n",
    "Asigna un entero a cada valor único. Asume un orden en las categorías. Variables ordinales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f891850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Hacer una copia para evitar cambiar los datos originales \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Aplicar codificador ordinal a cada columna con datos categóricos \n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n",
    "\n",
    "# De esta forma ha asigando valores aleatorios a las categorías. Asignar un orden puede mejorar el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dba9c7",
   "metadata": {},
   "source": [
    "#### 2.3.3- Codificación *One-Hot*\n",
    "Crea una columna por cada valor único e indica la presencia o no de ese valor. No asume un orden en las categorías. Variables nominales. No funciona bien si hay muchas categorias. Usar si hay 15 como máximo.\n",
    "\n",
    "* handle_unknown='ignore': para evitar errores cuando los datos de validación contienen clases que no están representadas en los datos de entrenamiento \n",
    "* sparse=False: asegura que las columnas codificadas se devuelvan como una matriz densa (en lugar de una matriz dispersa)\n",
    "\n",
    "Suele ser la mejor aproximación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f1827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Columnas que se codificarán one-hot (como máximo 10 categorias)\n",
    "low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "# Columnas que se eliminarán\n",
    "high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "\n",
    "\n",
    "# Aplicar codificador one-hot a cada columna con datos categóricos\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n",
    "\n",
    "# codificador one-hot elimina; ponerlo de nuevo \n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Eliminar columnas categóricas (se reemplazarán con codificación one-hot) \n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "#  añadir columnas codificadas one-hot a variables numéricas \n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1b52e",
   "metadata": {},
   "source": [
    "### Separar los datos para entrenar y validar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b38652",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_full.select_dtypes(exclude=['object']) # To keep things simple, we'll use only numerical predictors\n",
    "X_test = X_test_full.select_dtypes(exclude=['object'])\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7499ce9e",
   "metadata": {},
   "source": [
    "## 3- Seleccionar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor  # RandomForestRegressor\n",
    "\n",
    "modelo = DecisionTreeRegressor(random_state=1)    # escogemos el modelo Árbol de decisión\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d28642a",
   "metadata": {},
   "source": [
    "## 4- Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b668efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.fit(train_X, train_y)                      # se entrena el modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a5e1e",
   "metadata": {},
   "source": [
    "## 5- Evaluar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predicciones = modelo.predict(val_X)          # se hace predicción\n",
    "mean_absolute_error(val_y, val_predicciones)      # se evalua el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4222c411",
   "metadata": {},
   "source": [
    "### Underfitting and Overfitting\n",
    "\n",
    "   * *Underfitting*: no encuentra bien patrones relevantes, no predice bien\n",
    "   * *Overfitting*:  se ajusta demasiado a los datos de entrenamiento, no generaliza bien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34286ae4",
   "metadata": {},
   "source": [
    "## 6- Ajustar los parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d354d039",
   "metadata": {},
   "source": [
    "## 7- Generar la predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca17af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X_test = pd.DataFrame(final_imputer.transform(X_test))   # usar total de datos transformados\n",
    "\n",
    "# Sacar las predicciones con los datos para test \n",
    "preds_test = model.predict(final_X_test)\n",
    "\n",
    "\n",
    "# Guardar datos a archivo\n",
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'SalePrice': preds_test})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95601303",
   "metadata": {},
   "source": [
    "## Otros puntos a tener en cuenta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dda025",
   "metadata": {},
   "source": [
    "### Pipelines (canalizaciones)\n",
    "Para agrupar partes de código. Ventajas:\n",
    "- Código limpio\n",
    "- Menos errores\n",
    "- Más fácil de llevar a producción\n",
    "- Más opciones para validar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593bbcb8",
   "metadata": {},
   "source": [
    "#### 1- Definir pasos del preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bf1c6cc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numerical_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3961/2701167841.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m preprocessor = ColumnTransformer(\n\u001b[1;32m     17\u001b[0m     transformers=[\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m'num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumerical_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumerical_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numerical_cols' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Preprocesamiento de datos numéricos \n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocesamiento de datos categóricos\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Paquete de prepocesado\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5dfc9e",
   "metadata": {},
   "source": [
    "#### 2- Definir modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85346114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12749929",
   "metadata": {},
   "source": [
    "#### 3- Crear y evaluar el preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c5cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Agrupar código de preprocesamiento y modelado en una canalización \n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Procesamiento previo de datos de entrenamiento, modelo de ajuste\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Procesamiento previo de datos de validación, obtención de predicciones\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluar el modelo\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2413ca00",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "Divide los datos en subconjuntos, realiza el entrenamiento y validación alternando los subconjuntos y calcula la media al final.\n",
    "Para conjuntos de datos grandes es posible que no sea necesario. Si Cross-validation da resultados similares en cada subconjunto no es necesario hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n",
    "                              ('model', RandomForestRegressor(n_estimators=50,\n",
    "                                                              random_state=0))\n",
    "                             ])\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"MAE scores:\\n\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1c2d8",
   "metadata": {},
   "source": [
    "### Método XGBoost\n",
    "Consiste en un ciclo donde se van añadiendo modelos reduciéndose el error paulatinamente.\n",
    "<br> Usar con datos tabulados, no imagenes o video..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307cf7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "my_model = XGBRegressor()\n",
    "my_model.fit(X_train, y_train)\n",
    "\n",
    "predictions = my_model.predict(X_valid)\n",
    "print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0469022d",
   "metadata": {},
   "source": [
    "**n_estimators** = nº ciclos = nº modelos a añadir. Suele  estar entre 100-1000 (depende mucho del ratio de aprendizaje).\n",
    "<br>\n",
    "- muy bajo -> underfitting\n",
    "- muy alto -> overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4059a03",
   "metadata": {},
   "source": [
    "**early_stopping_rounds** encuentra automáticamente el valor óptimo de 'n_estimators'. \n",
    "Para el entrenamiento cuando el valor de validación deja de mejorar. Valor adecuado=5, para cuando lleva 5 ciclos empeorando la validación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0237b6",
   "metadata": {},
   "source": [
    "**learning_rate** = ratio de aprendizaje. Por defecto 0.1.  \n",
    "Por lo general, es mejor nº alto de 'n_estimators' y  bajo de 'learning_rate'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81071e12",
   "metadata": {},
   "source": [
    "**n_jobs** = para ejecución en paralelo de grandes conjuntos de datos = nº nucleos del ordenador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)], \n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a19e770",
   "metadata": {},
   "source": [
    "### Data leakage (fuga de datos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197c983",
   "metadata": {},
   "source": [
    "Ocurre cuando el conjunto de datos tiene información de la variable de destino.\n",
    "Produce alta precisión en el entrenamiento, pero muy baja en la predicciones reales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338ffa05",
   "metadata": {},
   "source": [
    "Tipos:\n",
    "- **target leakage**: cuando se incluyen datos que tienen información posterior a lo que se desea predecir\n",
    "- **train-test contamination**: ocurre cuando se mezclan los datos de entrenamiento y los de validación a la hora de entrenar el modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5284f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
